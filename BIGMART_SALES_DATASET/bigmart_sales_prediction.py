# -*- coding: utf-8 -*-
"""bigmart_sales_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MMEi3NV3YNWpDFp1cIvScfdKgWl71MTc
"""



"""Indroduction:

From the challange hosted at: https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/


About Company: The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined.BigMart will try to understand the properties of products and stores which play a key role in increasing sales. Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.
 
Data We have train (8523) and test (5681) data set, train data set has both input and output variable(s).Need to predict the sales for test data set.

Item_Identifier : Unique product ID.

Item_Weight : Weight of product.

Item_Fat_Content : Whether the product is low fat or not.

Item_Visibility : The % of total display area of all products in a store allocated to the particular product.

Item_Type : The category to which the product belongs.

Item_MRP : Maximum Retail Price (list price) of the product.

Outlet_Identifier : Unique store ID.

Outlet_Establishment_Year : The year in which store was established.

Outlet_Size : The size of the store in terms of ground area covered.

Outlet_Location_Type : The type of place in which the store is located.

Outlet_Type : Whether the outlet is just a grocery store or some sort of supermarket.

Item_Outlet_Sales : Sales of the product in the particulat store.

Purpose:
The aim is to build a predictive model and find out the sales of each product at a particular store.This is a supervised machine learning problem with target variable(Item_Outlet_Sales).

Exploratory Data Analysis:

Data cleaning:

1. columns which have missing values are:(Item_Weight,Outlet_Size).Imputed missing Values with sensible values as most counts of values and mode resplectively.

2. Item_Visibility has a min value of zero,which isn't possible because when a product is being sold at a store, the visibility cannot be 0.

3. Outlet_Establishment_Years vary from 1985 to 2009. The values might not be apt in this form. Rather, if we can convert them to how old the particular store is, it should have a better impact on sales.

4. Cleaning Item_Fat_Content column with two groups as low and regular.

5. Categorizating  Item_Visibility with 'Low Viz', 'Viz' and 'High Viz'.

6. Grouping Item_Type with Item_Id based on the object's first two character ('FD':'Food','NC':'Non-Consumable','DR':'Drinks').

7. Applying label encoding on the categorical variables and one-hot encoding. 

Data visualization:

1. Univariate Analysis

a) Numerical features:

1.  Item_MRP have the most positive correlation and the Item_Visibility have the lowest correlation with our target variable.

b) Categorical features:

1. Item_Type has 16 different types of unique values and it has high number for categorical variable.

2. There seems to be less number of stores with size equals to “High”.

3. Bigmart is a brand of medium and small size city compare to densely populated area.

4. There seems like Supermarket Type2 , Grocery Store and Supermarket Type3 all have low numbers of stores.

2. Bivariate Analysis

a) Numerical features:

1. As Item_Weight had a low correlation with our target variable. This plot shows there relation.

2. There seems to be no relation between the year of store establishment and the sales for the items.

b) Categorical features:

1. Low Fat products seem to higher sales than the Regular products.

2. Out of 10- There are 2 Groceries strore, 6 Supermarket Type1, 1Supermarket Type2, and 1 Supermarket Type3.

3. Most of the stores are of Supermarket Type1 of size High and they do not have best results. whereas Supermarket Type3 (OUT027) is a Medium size store and have best results.

4. Tier 2 cities have the higher sales than the Tier 1 and Tier 2.

Data Pre-processing:

1. choosing the Fat content, item vizibility bins, outlet size, loc type and type for LABEL ENCODER.

2. create dummies for outlet type.

3. drop all the object types features.

4. Feature Scaling.

Machine learning:

This is a supervised regression problem, as the target variable is numerical. Here logistic, Ridge, Lasso Regression which gives 56.41 % accuracy, Random Forest Regression which gives 56.63 % accuracy, Decision Tree Regression which gives 20.04 % accuracy.
So this doesn't predict sales on the test dataset.

Future Improvements:
Hyper-parameter Tuning and Gradient Boosting.
"""

#https://www.kaggle.com/devashish0507/big-mart-sales-prediction   -  from

#https://www.kaggle.com/chinmaymaganur/big-mart-sale-prediction-rmse-1164   --- eda
#https://www.kaggle.com/arindambaruah/predicting-the-big-mart-outlet-sales -- machine learning
#https://www.kaggle.com/saurabhmittal1996/big-mart-sales-prediction  ----eda(pivot table,visualization)
#https://www.kaggle.com/littleraj30/big-mart-sale-prediction-in-depth-ensemble   -----  eda with explaination

#https://www.analyticsvidhya.com/blog/2016/02/bigmart-sales-solution-top-20/
#https://www.kaggle.com/arindambaruah/predicting-the-big-mart-outlet-sales


##https://ai.plainenglish.io/big-mart-sales-prediction-project-tutorial-2de2414cdf2e  ----  eda



# import sklearn.metrics as sm
# from sklearn import linear_model, metrics
#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html
#reg_dt = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)
#reg_dt = DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,max_leaf_nodes=None, min_impurity_decrease=0.0,min_impurity_split=None, min_samples_leaf=1,min_samples_split=2, min_weight_fraction_leaf=0.0,presort=False, random_state=0, splitter='best')


#this code isn't working
#train_data['Item_Visibility_bins'] = train_data['Item_Visibility_bins'].replace(np.NaN, 'Low Viz')
#train_data['Item_Visibility_bins'] = train_data['Item_Visibility_bins'].fillna("Low Viz", inplace = True) 
#sns.countplot(train_data.Item_Type) 


# predictions = reg_rfr.predict(test_data)
# print(predictions)
#test_data['Item_Outlet_Sales_Prediction'] = predictions

# final = pd.DataFrame({"Item_Identifier":test_set["Item_Identifier"],"Outlet_Identifier":test_set["Outlet_Identifier"],"Item_Outlet_Sales":abs(predictions)})
# final.head(5)

# predict on new set with Decision Tree regression

# prediction = reg_dt.predict(test_data)
# # test_data['Item_Outlet_Sales_Prediction']=prediction
# # test_data.head(10)

# sets = pd.DataFrame({"Item_Identifier":test_set["Item_Identifier"],"Outlet_Identifier":test_set["Outlet_Identifier"],"Item_Outlet_Sales":prediction})
# sets.head(5)

#why is predict gives same value to the test data in python site:stackoverflow.com

#https://datascience.stackexchange.com/questions/46575/svr-is-giving-same-prediction-for-all-features   ----scaling for same prediction values of output

#import the files required - train
from google.colab import files
import io
uploaded =files.upload()
for fn in uploaded.keys():
   print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

#import the files required - test
upload =files.upload()
for fn in upload.keys():
   print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(upload[fn])))

# Commented out IPython magic to ensure Python compatibility.
#Load the required libraries 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

train_data = pd.read_csv('Train.csv')
print("Shape of train data - (Rows, columns): " + str(train_data.shape))

test_data = pd.read_csv('Test.csv')
print("Shape of test data - (Rows, columns): " + str(test_data.shape))

test_set = test_data.copy()

train_data.head()

train_data.info()

train_data.describe()

train_data.isnull().sum()/len(train_data)*100

# Plotting the percentage of missing values
total = train_data.isnull().sum().sort_values(ascending = False)
percent_total = (train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False)*100
missing = pd.concat([total, percent_total], axis=1, keys=["Total", "Percentage"])
missing_data = missing[missing['Total']>0]

plt.figure(figsize=(5,5))
sns.set(style="whitegrid")
sns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)
plt.title('Percentage of missing data by feature')
plt.xlabel('Features', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.show()

"""summary:
1. Item_Visibility has a min value of zero,which isn't possible because when a product is being sold at a store, the visibility cannot be 0.
2. Outlet_Establishment_Years vary from 1985 to 2009. The values might not be apt in this form. Rather, if we can convert them to how old the particular store is, it should have a better impact on sales.
3. There are some missing values which have to be treated.
4. Cleaning Item_Fat_Content column with two groups as low and regular.
5. Categorizating  Item_Visibility with 'Low Viz', 'Viz' and 'High Viz'.

#Exploratory data analysis
"""

num_features = train_data.select_dtypes(include=[np.number])
num_features.dtypes
#out of 12 there are only 5 numeric variables.

"""1. Univariate Analysis

a) Numerical features:
"""

corr=num_features.corr()
corr
corr['Item_Outlet_Sales'].sort_values(ascending=False)

"""b) Categorical features:"""

sns.countplot(x ='Item_Fat_Content', data = train_data)
plt.show()

sns.countplot(x ='Item_Type', data = train_data)
plt.xticks(rotation=90)
plt.show()

sns.countplot(x ='Outlet_Size', data = train_data)
plt.show()

sns.countplot(x ='Outlet_Location_Type', data = train_data)
plt.show()

sns.countplot(x ='Outlet_Type', data = train_data)
plt.xticks(rotation=90)
plt.show()

"""2. Bivariate Analysis

a) Numerical features:
"""

plt.figure(figsize=(12,7))
plt.xlabel("Item_Weight")
plt.ylabel("Item_Outlet_Sales")
plt.title("Item_Weight and Item_Outlet_Sales Analysis")
plt.plot(train_data.Item_Weight, train_data["Item_Outlet_Sales"],'.', alpha = 0.3)
plt.show()

Outlet_Establishment_Year_pivot = train_data.pivot_table(index='Outlet_Establishment_Year', values="Item_Outlet_Sales", aggfunc=np.median)
Outlet_Establishment_Year_pivot.plot(kind='bar', color='blue',figsize=(12,7))
plt.xlabel("Outlet_Establishment_Year")
plt.ylabel("Sqrt Item_Outlet_Sales")
plt.title("Impact of Outlet_Establishment_Year on Item_Outlet_Sales")
plt.show()

"""b) Categorical features:"""

train_data.pivot_table(index='Item_Fat_Content', values="Item_Outlet_Sales", aggfunc=np.median)

Item_Fat_Content_pivot = train_data.pivot_table(index='Item_Fat_Content', values="Item_Outlet_Sales", aggfunc=np.median)
Item_Fat_Content_pivot.plot(kind='bar', color='blue',figsize=(12,7))
plt.xlabel("Item_Fat_Content")
plt.ylabel("Item_Outlet_Sales")
plt.title("Impact of Item_Fat_Content on Item_Outlet_Sales")
plt.show()

train_data.pivot_table(values='Outlet_Type', columns='Outlet_Identifier',aggfunc=lambda x:x.mode())

Outlet_Identifier_pivot = train_data.pivot_table(index='Outlet_Identifier', values="Item_Outlet_Sales", aggfunc=np.median)
Outlet_Identifier_pivot.plot(kind='bar', color='blue',figsize=(12,7))
plt.xlabel("Outlet_Identifier")
plt.ylabel("Item_Outlet_Sales")
plt.title("Impact of Outlet_Identifier on Item_Outlet_Sales")
plt.show()

train_data.pivot_table(values='Outlet_Type',columns='Outlet_Size',aggfunc=lambda x:x.mode())

Outlet_Size_pivot = train_data.pivot_table(index='Outlet_Size', values="Item_Outlet_Sales", aggfunc=np.median)
Outlet_Size_pivot.plot(kind='bar', color='blue',figsize=(12,7))
plt.xlabel("Outlet_Size")
plt.ylabel("Item_Outlet_Sales")
plt.title("Impact of Outlet_Size on Item_Outlet_Sales")
plt.show()

Outlet_Location_Type_pivot = train_data.pivot_table(index='Outlet_Location_Type', values="Item_Outlet_Sales", aggfunc=np.median)
Outlet_Location_Type_pivot.plot(kind='bar', color='blue',figsize=(12,7))
plt.xlabel("Outlet_Location_Type")
plt.ylabel("Item_Outlet_Sales")
plt.title("Impact of Outlet_Location_Type on Item_Outlet_Sales")
plt.show()

"""#Data Pre-processing"""

# Filling the null values with the mean value
train_data['Item_Weight'] = train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean())

print(train_data['Outlet_Size'].unique())

print(train_data['Outlet_Size'].value_counts())

var = train_data.pivot_table(values = 'Outlet_Size', columns = 'Outlet_Type', aggfunc = (lambda x:x.mode()))
var

#Filling the null values with the 'medium' value from the above table
train_data['Outlet_Size'] = train_data['Outlet_Size'].fillna('Medium')

train_data.isnull().sum().sort_values(ascending = False)

print(train_data['Outlet_Establishment_Year'].unique())

#The data is from 2013 
train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']
train_data.head()

print(train_data['Item_Fat_Content'].unique())

train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace(['low fat', 'LF'], 'Low Fat')
train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace('reg', 'Regular')

train_data['Item_Fat_Content'].value_counts()

train_data['Item_Visibility'].value_counts()

train_data['Item_Visibility'].hist(bins=20)
plt.show()

# The minimum value of the item visibility feature is zero(0)
# Replace the minimum value with the 2nd minimum value of the feature, as item visibility cannot be zero
train_data['Item_Visibility'] = train_data['Item_Visibility'].replace(0.000000,0.003574698)

train_data['Item_Visibility_bins'] = pd.cut(train_data['Item_Visibility'], [0.000, 0.065, 0.13, 0.2], labels=['Low Viz', 'Viz', 'High Viz'])

train_data['Item_Visibility_bins'].value_counts()

train_data['Item_Visibility_bins'] = train_data['Item_Visibility_bins'].fillna('Low Viz')

train_data['Item_Visibility_bins'].value_counts()

train_data.isnull().sum().sort_values(ascending = False)

train_data['Item_Identifier'].value_counts(50)

"""These are the three catagories how the items are identified ['FD':'Food','NC':'Non-Consumable','DR':'Drinks']"""

train_data['Item_Type'].value_counts()

#grouping Item Id
train_data['Item_Id']=train_data['Item_Identifier'].str[:2]
train_data.groupby(['Item_Id','Item_Type'])['Item_Outlet_Sales'].count()

#Get the first two characters of ID:
train_data['Item_Type_Combined'] = train_data['Item_Identifier'].apply(lambda x: x[0:2])
#Rename them to more intuitive categories:
train_data['Item_Type_Combined'] = train_data['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drinks'})
train_data['Item_Type_Combined'].value_counts()

from sklearn.preprocessing import LabelEncoder
var_mod = ['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Item_Visibility_bins','Item_Type_Combined']
le = LabelEncoder()
for i in var_mod:
     train_data[i] = le.fit_transform(train_data[i])
train_data.dtypes

#create dummies for outlet type
dummy = pd.get_dummies(train_data['Outlet_Type'])
dummy.head()

train_data = pd.concat([train_data, dummy], axis=1)

# got to drop all the object types features
train_data = train_data.drop(['Item_Identifier', 'Item_Type', 'Outlet_Identifier', 'Outlet_Type','Outlet_Establishment_Year','Item_Id'], axis=1)

train_data.columns

"""#Data Pre-processing - test set"""

train_data.head()

test_data.isnull().sum()/len(test_data)*100

test_data['Item_Weight'] = test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean())

print(test_data['Outlet_Size'].unique())

print(test_data['Outlet_Size'].value_counts())

test_data['Outlet_Size'] = test_data['Outlet_Size'].fillna('Medium')

print(test_data['Outlet_Establishment_Year'].unique())

test_data['Outlet_Age'] = 2021 - test_data['Outlet_Establishment_Year']
test_data.head()

test_data.isnull().sum().sort_values(ascending = False)

print(test_data['Item_Fat_Content'].unique())

test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace(['low fat', 'LF'], 'Low Fat')

test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace('reg', 'Regular')

test_data['Item_Fat_Content'].value_counts()

test_data['Item_Visibility'].hist(bins=20)
plt.show()

test_data['Item_Visibility'] = test_data['Item_Visibility'].replace(0.000000,0.003574698)

test_data['Item_Visibility_bins'] = pd.cut(test_data['Item_Visibility'], [0.000, 0.065, 0.13, 0.2], labels=['Low Viz', 'Viz', 'High Viz'])

test_data['Item_Visibility_bins'].isnull().sum()

test_data['Item_Visibility_bins'] = test_data['Item_Visibility_bins'].fillna('Low Viz')

test_data.isnull().sum().sort_values(ascending = False)

test_data['Item_Type_Combined'] = test_data['Item_Identifier'].apply(lambda x: x[0:2])

test_data['Item_Type_Combined'] = test_data['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drinks'})
test_data['Item_Type_Combined'].value_counts()

from sklearn.preprocessing import LabelEncoder
var_mod = ['Item_Fat_Content','Outlet_Size','Outlet_Location_Type','Item_Visibility_bins','Item_Type_Combined']
le = LabelEncoder()
for i in var_mod:
     test_data[i] = le.fit_transform(test_data[i])
test_data.dtypes

dummy_var = pd.get_dummies(test_data['Outlet_Type'])
dummy_var.head()

test_data = pd.concat([test_data, dummy_var], axis=1)

test_data = test_data.drop(['Item_Identifier', 'Item_Type', 'Outlet_Identifier', 'Outlet_Type','Outlet_Establishment_Year'], axis=1)

test_data.columns

test_data.head()

test_data.isnull().sum().sort_values(ascending = False)

X = train_data.drop('Item_Outlet_Sales', axis=1)
y = train_data.Item_Outlet_Sales

"""#Machine learning"""

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 1/3, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_val = sc.fit_transform(X_val)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error

reg_lin=LogisticRegression(max_iter=2000)
reg_lin.fit(X_train,y_train)
reg_lin.score(X_train,y_train)

reg_lin_predict = reg_lin.predict(X_val)

rmse_lin=np.sqrt(mean_squared_error(reg_lin_predict,y_val))
print('RMSE for Linear Regression:{0:.2f}'.format(rmse_lin))


accuracy_lin = reg_lin.score(X_val,y_val)
print('Accuracy of the Linear Regression model:',accuracy_lin*100,'%' )

from sklearn.linear_model import RidgeCV
reg_rid=RidgeCV(cv=10)
reg_rid.fit(X_train,y_train)
reg_rid.score(X_train,y_train)

reg_rid_predict = reg_rid.predict(X_val)

rmse_rid=np.sqrt(mean_squared_error(reg_rid_predict,y_val))
print('RMSE for Ridge Regression:{0:.2f}'.format(rmse_rid))

accuracy_rid = reg_rid.score(X_val,y_val)
print('Accuracy of the Ridge Regression model:',accuracy_rid*100,'%' )

from sklearn.linear_model import Lasso
reg_lo=Lasso(alpha=0.01)
reg_lo.fit(X_train,y_train)
reg_lo.score(X_train,y_train)

reg_lo_predict = reg_lo.predict(X_val)

rmse_lo=np.sqrt(mean_squared_error(reg_lo_predict,y_val))
print('RMSE for Lasso Regression:{0:.2f}'.format(rmse_lo))

accuracy_lo = reg_lo.score(X_val,y_val)
print('Accuracy of the Lasso Regression model:',accuracy_lo*100,'%' )

from sklearn.ensemble import RandomForestRegressor
reg_rfr=RandomForestRegressor(random_state=0)
reg_rfr.fit(X_train,y_train)
reg_rfr.score(X_train,y_train)
reg_rfr_predict = reg_rfr.predict(X_val)

rmse_rfr=np.sqrt(mean_squared_error(reg_rfr_predict,y_val))
print('RMSE for Random Forest Regression:{0:.2f}'.format(rmse_rfr))

accuracy_rfr = reg_rfr.score(X_val,y_val)
print('Accuracy of the Random Forest Regression model:',accuracy_rfr*100,'%' )

from sklearn.tree import DecisionTreeRegressor
reg_dt = DecisionTreeRegressor(random_state=0)
reg_dt.fit(X_train,y_train)
reg_dt.score(X_train,y_train)
reg_dt_predict = reg_dt.predict(X_val)

rmse_dt = np.sqrt(mean_squared_error(reg_dt_predict,y_val))
print('RMSE for Decision Tree Regression:{0:.2f}'.format(rmse_dt))

accuracy_dt = reg_dt.score(X_val,y_val)
print('Accuracy of the Decision Tree Regression model:',accuracy_dt*100,'%' )